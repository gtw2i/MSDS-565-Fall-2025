{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d61d65f-23b1-445b-8544-500b5c6e0a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "pd.set_option('display.max_columns',100)\n",
    "pd.set_option('display.max_rows',100)\n",
    "\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad42da8a-fbc8-4c22-9498-f8a741310d3d",
   "metadata": {},
   "source": [
    "# Scrape data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af222ab7-fc8b-49a5-a323-6503d2a63c1a",
   "metadata": {},
   "source": [
    "### Read first webpage with list of computer scientists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b59d2816-7e14-4c98-b8b2-5276539b2690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read webpage and parse with bs4\n",
    "base_url = 'https://en.wikipedia.org/'\n",
    "list_url = 'wiki/List_of_dog_breeds'\n",
    "url = base_url + list_url\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Get names of scientists and links to their webpages\n",
    "links = {}\n",
    "div = soup.find('div', class_=\"mw-content-ltr mw-parser-output\")\n",
    "for ul in div.find_all('ul')[:5]:\n",
    "    for li in ul.find_all(\"li\"):\n",
    "        a = li.find(\"a\")\n",
    "        href = a['href']\n",
    "        links[a.text] = href\n",
    "        #print(a.text)\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c04b857-4ff3-4daf-af05-d722ef360837",
   "metadata": {},
   "source": [
    "# Get images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c23faf5-52f8-474e-99fb-e9ee1639fe7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Affenpinscher\n",
      "2 Afghan Hound\n",
      "3 Airedale Terrier\n",
      "4 Akita\n",
      "5 Alopekis\n",
      "6 American Cocker Spaniel\n",
      "7 American Eskimo Dog\n",
      "8 American Foxhound\n",
      "9 American Staffordshire Terrier\n",
      "10 Anglo-Français de Petite Vénerie\n",
      "11 Ariège Pointer\n",
      "12 Armant\n",
      "13 Australian Cattle Dog\n",
      "14 Australian Shepherd\n",
      "15 Austrian Pinscher\n",
      "16 Bắc Hà\n",
      "17 Barbet\n",
      "18 Basset Hound\n",
      "19 Bearded Collie\n",
      "20 Beauceron\n",
      "21 Bedlington Terrier\n",
      "22 Bichon Frisé\n",
      "23 Black Mouth Cur\n",
      "24 Blue Lacy\n",
      "25 Bolognese\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "img_dir = \"Dog Breeds/\"\n",
    "\n",
    "# dictionary of breeds and filenames\n",
    "breed_info = {}\n",
    "\n",
    "# Number of pages to scrape\n",
    "count = 0\n",
    "\n",
    "for breed_name, link in links.items():\n",
    "    # skip any failed requests\n",
    "    try:\n",
    "        # Read individual webpage\n",
    "        url = base_url + link\n",
    "        response = requests.get(url)\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        table = soup.find('table', class_=\"infobox\")\n",
    "        image = table.find(\"img\")\n",
    "    \n",
    "        # dont run if image is empty\n",
    "        if image:\n",
    "            url_img = \"https:\" + image[\"src\"]\n",
    "            image_response = requests.get(url_img, timeout=10)\n",
    "            \n",
    "            # raise error if bad status\n",
    "            image_response.raise_for_status()\n",
    "    \n",
    "            # create filepath\n",
    "            img_filename = \"_\".join(breed_name.split(\" \"))\n",
    "            img_filename = re.sub(r\"\\W\", \"\", img_filename) + \".png\"\n",
    "            img_filepath = img_dir + img_filename\n",
    "    \n",
    "            # download and save image\n",
    "            img = Image.open(BytesIO(image_response.content))\n",
    "            img.save(img_filepath, format=\"PNG\")\n",
    "            \n",
    "            breed_info[breed_name] = img_filepath\n",
    "        else:\n",
    "            1\n",
    "            #print(\"No image\")\n",
    "        # end\n",
    "\n",
    "        count += 1\n",
    "        print(count, breed_name)\n",
    "    except:\n",
    "        1\n",
    "    # end\n",
    "    \n",
    "    # Only read several pages\n",
    "    if count >= 25:\n",
    "        break\n",
    "    # end\n",
    "# end\n",
    "\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
