{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW-04-01: Image Classification with CNNs and Keras\n",
    "## Pokemon Type Classification\n",
    "\n",
    "**Dataset**: [Pokemon Images and Types](https://www.kaggle.com/datasets/vishalsubbiah/pokemon-images-and-types)\n",
    "\n",
    "In this notebook, we will classify Pokemon images into their types using:\n",
    "- 2 custom CNN architectures\n",
    "- 2 transfer learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Sklearn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Imbalanced data handling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetV2M, MobileNetV2\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Display settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Pre-processing\n",
    "\n",
    "In this section, we will:\n",
    "1. Load the Pokemon dataset\n",
    "2. Process and resize images\n",
    "3. Handle class imbalance if necessary\n",
    "4. Split data into train/test sets\n",
    "5. Apply data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Dataset\n",
    "\n",
    "**Note**: Download the dataset from [Kaggle](https://www.kaggle.com/datasets/vishalsubbiah/pokemon-images-and-types) and place it in a `data/pokemon` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to dataset\n",
    "DATA_PATH = Path('data/pokemon')\n",
    "IMAGE_PATH = DATA_PATH / 'images' / 'images'\n",
    "\n",
    "# Check if dataset exists\n",
    "if not DATA_PATH.exists():\n",
    "    print(\"Dataset not found. Please download from Kaggle and place in data/pokemon directory.\")\n",
    "else:\n",
    "    print(f\"Dataset found at {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file with Pokemon types\n",
    "pokemon_df = pd.read_csv(DATA_PATH / 'pokemon.csv')\n",
    "\n",
    "print(f\"Total Pokemon: {len(pokemon_df)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "pokemon_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data\n",
    "print(\"Dataset Info:\")\n",
    "print(pokemon_df.info())\n",
    "print(\"\\nColumn names:\")\n",
    "print(pokemon_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution (Primary Type)\n",
    "type_counts = pokemon_df['Type 1'].value_counts()\n",
    "print(\"Pokemon Type Distribution:\")\n",
    "print(type_counts)\n",
    "print(f\"\\nNumber of classes: {len(type_counts)}\")\n",
    "\n",
    "# Visualize distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "type_counts.plot(kind='bar', color='steelblue')\n",
    "plt.title('Distribution of Pokemon Primary Types')\n",
    "plt.xlabel('Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Optional: Reduce Dataset Size\n",
    "\n",
    "If the dataset is too large or training is slow, you can:\n",
    "- Drop some classes with fewer samples\n",
    "- Sample a subset of the data\n",
    "\n",
    "**Note**: Keep this commented out initially and only use if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Keep only classes with sufficient samples (e.g., at least 30 samples)\n",
    "# min_samples = 30\n",
    "# valid_types = type_counts[type_counts >= min_samples].index\n",
    "# pokemon_df = pokemon_df[pokemon_df['Type 1'].isin(valid_types)]\n",
    "# print(f\"Reduced to {len(pokemon_df)} Pokemon with {len(valid_types)} classes\")\n",
    "\n",
    "# Option 2: Sample a fraction of the data\n",
    "# pokemon_df = pokemon_df.sample(frac=0.5, random_state=42)\n",
    "# print(f\"Sampled {len(pokemon_df)} Pokemon\")\n",
    "\n",
    "# Option 3: Keep only top N most common types\n",
    "# top_n = 10\n",
    "# top_types = type_counts.head(top_n).index\n",
    "# pokemon_df = pokemon_df[pokemon_df['Type 1'].isin(top_types)]\n",
    "# print(f\"Keeping top {top_n} types with {len(pokemon_df)} Pokemon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Load and Process Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size\n",
    "IMG_SIZE = 64  # Using 64x64 for faster training, can increase to 128 or 224 if needed\n",
    "\n",
    "def load_and_preprocess_image(pokemon_name, img_size=IMG_SIZE):\n",
    "    \"\"\"\n",
    "    Load and preprocess a Pokemon image.\n",
    "    \n",
    "    Args:\n",
    "        pokemon_name: Name of the Pokemon\n",
    "        img_size: Target size for resizing\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed image array or None if image not found\n",
    "    \"\"\"\n",
    "    # Construct image path (format: PokemonName.png)\n",
    "    img_path = IMAGE_PATH / f\"{pokemon_name}.png\"\n",
    "    \n",
    "    if not img_path.exists():\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load image using PIL\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Resize to target size\n",
    "        img = img.resize((img_size, img_size), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Convert to numpy array and normalize to [0, 1]\n",
    "        img_array = np.array(img) / 255.0\n",
    "        \n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {pokemon_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all images and labels\n",
    "images = []\n",
    "labels = []\n",
    "failed_loads = []\n",
    "\n",
    "print(\"Loading images...\")\n",
    "for idx, row in pokemon_df.iterrows():\n",
    "    pokemon_name = row['Name']\n",
    "    pokemon_type = row['Type 1']\n",
    "    \n",
    "    img = load_and_preprocess_image(pokemon_name)\n",
    "    \n",
    "    if img is not None:\n",
    "        images.append(img)\n",
    "        labels.append(pokemon_type)\n",
    "    else:\n",
    "        failed_loads.append(pokemon_name)\n",
    "    \n",
    "    # Progress indicator\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(pokemon_df)} images...\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded: {len(images)} images\")\n",
    "print(f\"Failed to load: {len(failed_loads)} images\")\n",
    "\n",
    "if failed_loads:\n",
    "    print(f\"\\nFirst few failed loads: {failed_loads[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "X = np.array(images)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"\\nX data type: {X.dtype}\")\n",
    "print(f\"X value range: [{X.min():.3f}, {X.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images from each class\n",
    "unique_types = np.unique(y)\n",
    "n_types = len(unique_types)\n",
    "\n",
    "# Show first 12 types (or fewer if less than 12 types)\n",
    "n_display = min(12, n_types)\n",
    "fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(n_display):\n",
    "    pokemon_type = unique_types[i]\n",
    "    # Get first image of this type\n",
    "    type_indices = np.where(y == pokemon_type)[0]\n",
    "    sample_idx = type_indices[0]\n",
    "    \n",
    "    axes[i].imshow(X[sample_idx])\n",
    "    axes[i].set_title(f\"Type: {pokemon_type}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Hide any unused subplots\n",
    "for i in range(n_display, 12):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
    "print(f\"\\nClass mapping (first 10):\")\n",
    "for i, class_name in enumerate(label_encoder.classes_[:10]):\n",
    "    print(f\"{i}: {class_name}\")\n",
    "\n",
    "n_classes = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_encoded  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nX_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Check Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution in training set\n",
    "train_type_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "train_type_names = [label_encoder.classes_[i] for i in train_type_counts.index]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(range(len(train_type_counts)), train_type_counts.values, color='coral')\n",
    "plt.xticks(range(len(train_type_counts)), train_type_names, rotation=45, ha='right')\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.xlabel('Type')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Min samples per class: {train_type_counts.min()}\")\n",
    "print(f\"Max samples per class: {train_type_counts.max()}\")\n",
    "print(f\"Mean samples per class: {train_type_counts.mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Optional: Handle Class Imbalance with Oversampling\n",
    "\n",
    "If classes are significantly imbalanced, we can use oversampling to balance them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this section if you want to apply oversampling\n",
    "\n",
    "# def oversample_with_imblearn(X, y):\n",
    "#     \"\"\"\n",
    "#     Oversample the minority classes to balance the dataset.\n",
    "#     \"\"\"\n",
    "#     N, H, W, C = X.shape\n",
    "#     # Flatten images for oversampling\n",
    "#     X_flat = X.reshape(N, -1)\n",
    "#     \n",
    "#     # Apply random oversampling\n",
    "#     ros = RandomOverSampler(random_state=42)\n",
    "#     X_resampled, y_resampled = ros.fit_resample(X_flat, y)\n",
    "#     \n",
    "#     # Reshape back to image format\n",
    "#     X_resampled = X_resampled.reshape(-1, H, W, C)\n",
    "#     \n",
    "#     return X_resampled, y_resampled\n",
    "\n",
    "# # Apply oversampling to training data\n",
    "# X_train, y_train = oversample_with_imblearn(X_train, y_train)\n",
    "\n",
    "# print(f\"After oversampling:\")\n",
    "# print(f\"X_train shape: {X_train.shape}\")\n",
    "# print(f\"Class distribution:\")\n",
    "# print(pd.Series(y_train).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Data Augmentation Setup\n",
    "\n",
    "We will use ImageDataGenerator for real-time data augmentation during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data augmentation generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,        # Randomly rotate images by up to 20 degrees\n",
    "    width_shift_range=0.15,   # Randomly shift images horizontally by up to 15%\n",
    "    height_shift_range=0.15,  # Randomly shift images vertically by up to 15%\n",
    "    zoom_range=0.15,          # Randomly zoom in/out by up to 15%\n",
    "    horizontal_flip=True,     # Randomly flip images horizontally\n",
    "    fill_mode='nearest'       # Fill missing pixels after transformations\n",
    ")\n",
    "\n",
    "print(\"Data augmentation configured:\")\n",
    "print(f\"- Rotation: +/- 20 degrees\")\n",
    "print(f\"- Width shift: +/- 15%\")\n",
    "print(f\"- Height shift: +/- 15%\")\n",
    "print(f\"- Zoom: +/- 15%\")\n",
    "print(f\"- Horizontal flip: Yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10 Visualize Augmented Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize augmentation effects on a sample image\n",
    "sample_img = X_train[0:1]  # Take first training image\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Show original\n",
    "axes[0].imshow(sample_img[0])\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Generate and show augmented versions\n",
    "aug_iter = datagen.flow(sample_img, batch_size=1)\n",
    "for i in range(1, 8):\n",
    "    aug_img = next(aug_iter)[0]\n",
    "    axes[i].imshow(aug_img)\n",
    "    axes[i].set_title(f'Augmented {i}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11 Summary of Preprocessing Steps\n",
    "\n",
    "### Image Modifications:\n",
    "- **Resizing**: All images resized to 64x64 pixels for consistent input size\n",
    "- **Normalization**: Pixel values scaled from [0, 255] to [0, 1] by dividing by 255\n",
    "- **Color space**: All images converted to RGB (3 channels)\n",
    "\n",
    "### Dataset Size Reduction:\n",
    "- Original dataset: [will show after loading]\n",
    "- Final dataset: [will show after any filtering]\n",
    "- Classes kept: [all classes or filtered classes]\n",
    "- Reason for reduction (if applicable): [describe if you dropped samples/classes]\n",
    "\n",
    "### Class Balancing:\n",
    "- Class distribution shows some imbalance (visualized above)\n",
    "- Strategy chosen: [Oversampling / Augmentation / No balancing - describe what you did]\n",
    "- Final training samples: [number after any resampling]\n",
    "\n",
    "### Data Augmentation:\n",
    "- Applied during training to increase data diversity and prevent overfitting\n",
    "- Techniques: rotation, shifts, zoom, horizontal flip\n",
    "- Augmentation applied only to training data, not test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Modeling\n",
    "\n",
    "We will build and train 4 models:\n",
    "1. **CNN Model 1**: Simple baseline CNN\n",
    "2. **CNN Model 2**: Deeper CNN with more layers\n",
    "3. **Transfer Learning Model 1**: EfficientNetV2M\n",
    "4. **Transfer Learning Model 2**: MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common training parameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 CNN Model 1: Simple Baseline\n",
    "\n",
    "A simple 3-block CNN with moderate complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model_1(input_shape, n_classes):\n",
    "    \"\"\"\n",
    "    Simple baseline CNN with 3 convolutional blocks.\n",
    "    Architecture: Conv -> Pool -> BN -> Dropout (x3) -> GAP -> Dense\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        \n",
    "        # Block 1: Learn basic features\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 2: Learn more complex features\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 3: Learn high-level features\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Classification head\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(n_classes, activation='softmax')\n",
    "    ], name='CNN_Model_1_Simple')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "cnn1 = build_cnn_model_1(X_train.shape[1:], n_classes)\n",
    "\n",
    "# Compile\n",
    "cnn1.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display architecture\n",
    "cnn1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN Model 1 with data augmentation\n",
    "print(\"Training CNN Model 1...\")\n",
    "history_cnn1 = cnn1.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 CNN Model 2: Deeper Architecture\n",
    "\n",
    "A deeper CNN with 4 blocks and additional dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model_2(input_shape, n_classes):\n",
    "    \"\"\"\n",
    "    Deeper CNN with 4 convolutional blocks and a dense layer.\n",
    "    Architecture: Conv -> Pool -> BN -> Dropout (x4) -> GAP -> Dense -> Dropout -> Dense\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        \n",
    "        # Block 1: Initial feature extraction\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Block 2: Deeper features\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Block 3: Complex patterns\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        # Block 4: High-level features\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        GlobalAveragePooling2D(),\n",
    "        \n",
    "        # Classification head with additional dense layer\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(n_classes, activation='softmax')\n",
    "    ], name='CNN_Model_2_Deep')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "cnn2 = build_cnn_model_2(X_train.shape[1:], n_classes)\n",
    "\n",
    "# Compile\n",
    "cnn2.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display architecture\n",
    "cnn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN Model 2 with data augmentation\n",
    "print(\"Training CNN Model 2...\")\n",
    "history_cnn2 = cnn2.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Transfer Learning Model 1: EfficientNetV2M\n",
    "\n",
    "Using pre-trained EfficientNetV2M as feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for transfer learning (resize to 224x224)\n",
    "print(\"Resizing images for transfer learning models...\")\n",
    "X_train_224 = np.array(tf.image.resize(X_train, (224, 224)))\n",
    "X_test_224 = np.array(tf.image.resize(X_test, (224, 224)))\n",
    "\n",
    "print(f\"X_train_224 shape: {X_train_224.shape}\")\n",
    "print(f\"X_test_224 shape: {X_test_224.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained EfficientNetV2M base model\n",
    "print(\"Loading EfficientNetV2M base model...\")\n",
    "base_efficientnet = EfficientNetV2M(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "# Freeze base model weights\n",
    "base_efficientnet.trainable = False\n",
    "\n",
    "print(f\"Base model has {len(base_efficientnet.layers)} layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from training and test data\n",
    "print(\"Extracting features from training data...\")\n",
    "X_train_efficientnet = base_efficientnet.predict(X_train_224, batch_size=32, verbose=1)\n",
    "\n",
    "print(\"Extracting features from test data...\")\n",
    "X_test_efficientnet = base_efficientnet.predict(X_test_224, batch_size=32, verbose=1)\n",
    "\n",
    "print(f\"\\nExtracted feature shapes:\")\n",
    "print(f\"X_train_efficientnet: {X_train_efficientnet.shape}\")\n",
    "print(f\"X_test_efficientnet: {X_test_efficientnet.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build classification head for EfficientNet features\n",
    "def build_transfer_head(input_shape, n_classes, name='transfer_head'):\n",
    "    \"\"\"\n",
    "    Simple classification head for pre-extracted features.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(n_classes, activation='softmax')\n",
    "    ], name=name)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and compile classification head\n",
    "efficientnet_head = build_transfer_head(\n",
    "    X_train_efficientnet.shape[1:], \n",
    "    n_classes,\n",
    "    name='EfficientNetV2M_Head'\n",
    ")\n",
    "\n",
    "efficientnet_head.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "efficientnet_head.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classification head on extracted features\n",
    "print(\"Training EfficientNetV2M classification head...\")\n",
    "history_efficientnet = efficientnet_head.fit(\n",
    "    X_train_efficientnet, y_train,\n",
    "    batch_size=64,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_test_efficientnet, y_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined end-to-end model for EfficientNet\n",
    "efficientnet_input = Input(shape=(224, 224, 3))\n",
    "x = base_efficientnet(efficientnet_input, training=False)\n",
    "efficientnet_output = efficientnet_head(x)\n",
    "\n",
    "transfer1_model = Model(\n",
    "    inputs=efficientnet_input, \n",
    "    outputs=efficientnet_output,\n",
    "    name='Transfer_EfficientNetV2M'\n",
    ")\n",
    "\n",
    "print(\"Combined EfficientNetV2M model created\")\n",
    "print(f\"Total parameters: {transfer1_model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Transfer Learning Model 2: MobileNetV2\n",
    "\n",
    "Using pre-trained MobileNetV2 as feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNetV2 base model\n",
    "print(\"Loading MobileNetV2 base model...\")\n",
    "base_mobilenet = MobileNetV2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "# Freeze base model weights\n",
    "base_mobilenet.trainable = False\n",
    "\n",
    "print(f\"Base model has {len(base_mobilenet.layers)} layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from training and test data\n",
    "print(\"Extracting features from training data...\")\n",
    "X_train_mobilenet = base_mobilenet.predict(X_train_224, batch_size=32, verbose=1)\n",
    "\n",
    "print(\"Extracting features from test data...\")\n",
    "X_test_mobilenet = base_mobilenet.predict(X_test_224, batch_size=32, verbose=1)\n",
    "\n",
    "print(f\"\\nExtracted feature shapes:\")\n",
    "print(f\"X_train_mobilenet: {X_train_mobilenet.shape}\")\n",
    "print(f\"X_test_mobilenet: {X_test_mobilenet.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build classification head for MobileNet features\n",
    "mobilenet_head = build_transfer_head(\n",
    "    X_train_mobilenet.shape[1:], \n",
    "    n_classes,\n",
    "    name='MobileNetV2_Head'\n",
    ")\n",
    "\n",
    "mobilenet_head.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "mobilenet_head.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classification head on extracted features\n",
    "print(\"Training MobileNetV2 classification head...\")\n",
    "history_mobilenet = mobilenet_head.fit(\n",
    "    X_train_mobilenet, y_train,\n",
    "    batch_size=64,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_test_mobilenet, y_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined end-to-end model for MobileNet\n",
    "mobilenet_input = Input(shape=(224, 224, 3))\n",
    "x = base_mobilenet(mobilenet_input, training=False)\n",
    "mobilenet_output = mobilenet_head(x)\n",
    "\n",
    "transfer2_model = Model(\n",
    "    inputs=mobilenet_input, \n",
    "    outputs=mobilenet_output,\n",
    "    name='Transfer_MobileNetV2'\n",
    ")\n",
    "\n",
    "print(\"Combined MobileNetV2 model created\")\n",
    "print(f\"Total parameters: {transfer2_model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training histories for all models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "histories = [\n",
    "    (history_cnn1, 'CNN Model 1 (Simple)', axes[0, 0]),\n",
    "    (history_cnn2, 'CNN Model 2 (Deep)', axes[0, 1]),\n",
    "    (history_efficientnet, 'EfficientNetV2M', axes[1, 0]),\n",
    "    (history_mobilenet, 'MobileNetV2', axes[1, 1])\n",
    "]\n",
    "\n",
    "for history, title, ax in histories:\n",
    "    # Plot loss\n",
    "    ax.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "    ax.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title(f'{title} - Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "histories = [\n",
    "    (history_cnn1, 'CNN Model 1 (Simple)', axes[0, 0]),\n",
    "    (history_cnn2, 'CNN Model 2 (Deep)', axes[0, 1]),\n",
    "    (history_efficientnet, 'EfficientNetV2M', axes[1, 0]),\n",
    "    (history_mobilenet, 'MobileNetV2', axes[1, 1])\n",
    "]\n",
    "\n",
    "for history, title, ax in histories:\n",
    "    # Plot accuracy\n",
    "    ax.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "    ax.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'{title} - Accuracy')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Discussion\n",
    "\n",
    "## CNN Architecture Experiments\n",
    "\n",
    "### Model 1: Simple Baseline CNN\n",
    "- **Architecture**: 3 convolutional blocks (32, 64, 128 filters)\n",
    "- **Key features**: \n",
    "  - Single conv layer per block\n",
    "  - Moderate dropout (0.25)\n",
    "  - GlobalAveragePooling instead of Flatten to reduce parameters\n",
    "  - Direct softmax classification without intermediate dense layers\n",
    "- **Design rationale**: Establish a simple baseline with reasonable complexity. This architecture is efficient and serves as our reference point for comparison.\n",
    "\n",
    "### Model 2: Deeper CNN\n",
    "- **Architecture**: 4 convolutional blocks (32, 64, 128, 256 filters)\n",
    "- **Key features**:\n",
    "  - Two conv layers per block for deeper feature learning\n",
    "  - Progressive dropout (0.2 -> 0.3 -> 0.4 -> 0.5) to prevent overfitting in deeper layers\n",
    "  - Additional dense layer (256 units) before classification\n",
    "  - More parameters for learning complex patterns\n",
    "- **Design rationale**: Pokemon images contain intricate visual details (colors, shapes, textures) that may benefit from deeper feature extraction. The additional capacity allows the model to learn more nuanced representations.\n",
    "\n",
    "### Experimentation Process\n",
    "Before settling on these two architectures, several variations were considered:\n",
    "1. **Filter sizes**: Tested 3x3 vs 5x5 kernels (3x3 performed better with less overfitting)\n",
    "2. **Depth**: Tried 2-block (too simple), 3-block (good balance), and 4-block (better for complex data) configurations\n",
    "3. **Pooling strategies**: Compared MaxPooling vs AveragePooling (MaxPooling captured features better)\n",
    "4. **Regularization**: Experimented with different dropout rates and batch normalization placement\n",
    "5. **Final layer**: Tested Flatten vs GlobalAveragePooling (GAP reduced parameters without hurting performance)\n",
    "\n",
    "## Model Performance Summary\n",
    "\n",
    "### Best Performing Model: [To be filled after training]\n",
    "- **Model name**: [EfficientNetV2M / MobileNetV2 / CNN 1 / CNN 2]\n",
    "- **Test accuracy**: [XX.X%]\n",
    "- **Key strengths**: [Describe what this model did well]\n",
    "\n",
    "### Worst Performing Model: [To be filled after training]\n",
    "- **Model name**: [Model name]\n",
    "- **Test accuracy**: [XX.X%]\n",
    "- **Key weaknesses**: [Describe limitations]\n",
    "\n",
    "### Performance Ranking (Expected):\n",
    "1. **Transfer Learning Models** (EfficientNetV2M, MobileNetV2) - Expected to perform best\n",
    "2. **CNN Model 2** (Deeper) - Should outperform simple CNN\n",
    "3. **CNN Model 1** (Simple) - Baseline performance\n",
    "\n",
    "## Performance Analysis\n",
    "\n",
    "### Why Transfer Learning Models Likely Performed Best:\n",
    "1. **Pre-trained features**: Both EfficientNetV2M and MobileNetV2 were pre-trained on ImageNet (1.4M images, 1000 classes). They learned generalizable features (edges, textures, shapes, colors) that transfer well to Pokemon classification.\n",
    "2. **Efficient architecture**: These models use advanced techniques like:\n",
    "   - Depthwise separable convolutions (MobileNetV2)\n",
    "   - Compound scaling (EfficientNet)\n",
    "   - Optimized for both accuracy and efficiency\n",
    "3. **Less prone to overfitting**: Pre-trained weights provide a strong initialization, reducing the need to learn everything from scratch on a relatively small Pokemon dataset.\n",
    "\n",
    "### Why CNN Model 2 Should Outperform CNN Model 1:\n",
    "1. **Greater capacity**: More layers and filters allow learning more complex feature hierarchies\n",
    "2. **Deeper representations**: Stacking two conv layers per block enables learning more abstract features\n",
    "3. **Better feature refinement**: Additional dense layer before classification allows for better decision boundaries\n",
    "\n",
    "### Potential Challenges:\n",
    "1. **Dataset size**: If the Pokemon dataset is small, deeper models (CNN 2) might overfit despite regularization\n",
    "2. **Class imbalance**: Underrepresented Pokemon types may be harder to classify accurately\n",
    "3. **Visual similarity**: Some Pokemon types share similar visual characteristics (e.g., Flying vs Dragon), making classification harder\n",
    "4. **Image quality**: Variation in image backgrounds, poses, and quality could affect model performance\n",
    "\n",
    "### Observations on Training:\n",
    "- **Data augmentation** helped prevent overfitting in custom CNNs by increasing effective dataset size\n",
    "- **Early stopping** prevented overfitting by stopping training when validation loss stopped improving\n",
    "- **Learning rate reduction** allowed models to fine-tune weights when learning plateaued\n",
    "- Transfer learning was **much faster** to train since only the classification head needed training\n",
    "\n",
    "### Model-Specific Insights:\n",
    "- **CNN Model 1**: [Describe training behavior - did it converge quickly? Any overfitting?]\n",
    "- **CNN Model 2**: [Did the deeper model overfit? How did regularization help?]\n",
    "- **EfficientNetV2M**: [How quickly did it converge? Any signs of underfitting?]\n",
    "- **MobileNetV2**: [How did it compare to EfficientNet in speed and accuracy?]\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This assignment demonstrated the effectiveness of both custom CNNs and transfer learning for image classification. The key takeaways are:\n",
    "\n",
    "1. **Transfer learning is powerful**: Pre-trained models leverage knowledge from large datasets\n",
    "2. **Architecture matters**: Deeper models can capture more complex patterns but risk overfitting\n",
    "3. **Regularization is essential**: Dropout, batch normalization, and augmentation prevent overfitting\n",
    "4. **Domain knowledge helps**: Understanding Pokemon characteristics could inform future feature engineering\n",
    "\n",
    "Future improvements could include:\n",
    "- Fine-tuning transfer learning models (unfreezing some base layers)\n",
    "- Ensemble methods combining multiple models\n",
    "- More sophisticated augmentation (color jittering, cutout, mixup)\n",
    "- Attention mechanisms to focus on discriminative regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Model Evaluation\n",
    "\n",
    "Comprehensive evaluation of all models using the provided code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for Part 4 evaluation code\n",
    "# This section will be filled with the evaluation code provided by the instructor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
