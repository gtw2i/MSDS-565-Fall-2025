{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# HW-04-01: Pokemon Image Classification - Preprocessing\n",
    "\n",
    "**Dataset:** [Pokemon Images and Types](https://www.kaggle.com/datasets/vishalsubbiah/pokemon-images-and-types)\n",
    "\n",
    "This notebook preprocesses the Pokemon dataset for CNN classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download_header",
   "metadata": {},
   "source": [
    "## Step 1: Download Dataset\n",
    "\n",
    "**Option A: Kaggle CLI**\n",
    "```bash\n",
    "pip install kaggle\n",
    "kaggle datasets download -d vishalsubbiah/pokemon-images-and-types\n",
    "unzip pokemon-images-and-types.zip -d ../data/pokemon/\n",
    "```\n",
    "\n",
    "**Option B: Manual Download**\n",
    "1. Go to https://www.kaggle.com/datasets/vishalsubbiah/pokemon-images-and-types\n",
    "2. Download and extract to `../data/pokemon/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_header",
   "metadata": {},
   "source": [
    "## Step 2: Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pokemon_images(data_dir, img_size=(120, 120)):\n",
    "    \"\"\"\n",
    "    Load Pokemon images from directory structure.\n",
    "    Assumes: data_dir/type_name/pokemon_image.png\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Get class directories\n",
    "    class_dirs = sorted([d for d in data_dir.iterdir() if d.is_dir()])\n",
    "    label_names = [d.name for d in class_dirs]\n",
    "    \n",
    "    print(f\"Found {len(label_names)} classes: {label_names}\")\n",
    "    \n",
    "    # Load images from each class\n",
    "    for label_idx, class_dir in enumerate(tqdm(class_dirs, desc=\"Loading images\")):\n",
    "        image_files = list(class_dir.glob('*.png')) + list(class_dir.glob('*.jpg'))\n",
    "        \n",
    "        for img_file in image_files:\n",
    "            try:\n",
    "                img = Image.open(img_file).convert('RGB')\n",
    "                img = img.resize(img_size)\n",
    "                images.append(np.array(img))\n",
    "                labels.append(label_idx)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_file}: {e}\")\n",
    "    \n",
    "    return np.array(images), np.array(labels), label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_dir = Path('../data/pokemon')\n",
    "\n",
    "if not data_dir.exists():\n",
    "    print(f\"⚠️  Data directory not found: {data_dir}\")\n",
    "    print(\"Please download the dataset first\")\n",
    "else:\n",
    "    images, labels, label_names = load_pokemon_images(data_dir, img_size=(120, 120))\n",
    "    \n",
    "    print(f\"\\nLoaded {len(images)} images\")\n",
    "    print(f\"Image shape: {images.shape}\")\n",
    "    print(f\"Number of classes: {len(label_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize_header",
   "metadata": {},
   "source": [
    "## Step 3: Visualize Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random samples\n",
    "fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "axes = axes.ravel()\n",
    "\n",
    "random_indices = np.random.choice(len(images), size=9, replace=False)\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    axes[i].imshow(images[idx])\n",
    "    axes[i].set_title(label_names[labels[idx]])\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normalize_header",
   "metadata": {},
   "source": [
    "## Step 4: Normalize Pixel Values\n",
    "\n",
    "Convert pixel values from [0, 255] to [0, 1] for neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normalize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to [0, 1] range\n",
    "X = images.astype('float32') / 255.0\n",
    "y = labels\n",
    "\n",
    "print(f\"Before normalization: min={images.min()}, max={images.max()}\")\n",
    "print(f\"After normalization:  min={X.min()}, max={X.max()}\")\n",
    "print(f\"\\nData shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split_header",
   "metadata": {},
   "source": [
    "## Step 5: Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test sets (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} images\")\n",
    "print(f\"Test set:     {X_test.shape[0]} images\")\n",
    "print(f\"\\nClass distribution (train):\")\n",
    "for idx, name in enumerate(label_names):\n",
    "    count = np.sum(y_train == idx)\n",
    "    print(f\"  {name}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_header",
   "metadata": {},
   "source": [
    "## Step 6: Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to compressed numpy format\n",
    "output_dir = Path('../data/pokemon_preprocessed')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.savez_compressed(\n",
    "    output_dir / 'pokemon_data.npz',\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    label_names=label_names\n",
    ")\n",
    "\n",
    "file_size = (output_dir / 'pokemon_data.npz').stat().st_size / (1024**2)\n",
    "print(f\"✓ Data saved to {output_dir / 'pokemon_data.npz'}\")\n",
    "print(f\"  File size: {file_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_later_header",
   "metadata": {},
   "source": [
    "## Loading Preprocessed Data (for training notebook)\n",
    "\n",
    "Use this code in your training notebook to load the preprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_function_later",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "data = np.load('../data/pokemon_preprocessed/pokemon_data.npz')\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "label_names = data['label_names']\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Preprocessing steps completed:**\n",
    "1. Loaded Pokemon images from directory structure\n",
    "2. Resized images to 120×120 pixels\n",
    "3. Normalized pixel values to [0, 1] range\n",
    "4. Split data into train/test sets (80/20)\n",
    "5. Saved preprocessed data for CNN training\n",
    "\n",
    "**Next step:** Build and train a CNN model using the preprocessed data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
