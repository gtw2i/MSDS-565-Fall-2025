{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# HW-04-01: Image Classification with CNNs and Keras\n",
    "## Pokemon Type Classification - Data Preprocessing\n",
    "\n",
    "**Dataset:** [Pokemon Images and Types](https://www.kaggle.com/datasets/vishalsubbiah/pokemon-images-and-types)\n",
    "\n",
    "This notebook demonstrates preprocessing steps for the Pokemon image classification task following the pattern established in the CIFAR-10 CNN examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download_instructions",
   "metadata": {},
   "source": [
    "## Step 1: Download the Dataset\n",
    "\n",
    "### Option A: Using Kaggle CLI (Recommended)\n",
    "\n",
    "First, install the Kaggle package (if not already installed):\n",
    "```bash\n",
    "pip install kaggle\n",
    "```\n",
    "\n",
    "Then, set up your Kaggle API credentials:\n",
    "1. Go to https://www.kaggle.com/account\n",
    "2. Scroll to 'API' section and click 'Create New Token'\n",
    "3. This downloads `kaggle.json`\n",
    "4. Place it in `~/.kaggle/kaggle.json` (Linux/Mac) or `C:\\Users\\<Windows-username>\\.kaggle\\kaggle.json` (Windows)\n",
    "5. Set permissions: `chmod 600 ~/.kaggle/kaggle.json`\n",
    "\n",
    "Download the dataset:\n",
    "```bash\n",
    "kaggle datasets download -d vishalsubbiah/pokemon-images-and-types\n",
    "unzip pokemon-images-and-types.zip -d ../data/pokemon/\n",
    "```\n",
    "\n",
    "### Option B: Manual Download\n",
    "\n",
    "1. Visit https://www.kaggle.com/datasets/vishalsubbiah/pokemon-images-and-types\n",
    "2. Click 'Download' button\n",
    "3. Extract the zip file to `../data/pokemon/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download using Kaggle CLI\n",
    "# !kaggle datasets download -d vishalsubbiah/pokemon-images-and-types\n",
    "# !unzip -q pokemon-images-and-types.zip -d ../data/pokemon/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explore_structure",
   "metadata": {},
   "source": [
    "## Step 2: Explore Dataset Structure\n",
    "\n",
    "**Preprocessing Step 1: Understanding the Data Organization**\n",
    "\n",
    "Before we can preprocess images, we need to understand:\n",
    "- How files are organized (folders by class/type?)\n",
    "- Image formats and sizes\n",
    "- Number of classes and samples per class\n",
    "- Any metadata files (CSV with labels, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_path",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "data_dir = Path('../data/pokemon')\n",
    "\n",
    "# Check if data directory exists\n",
    "if not data_dir.exists():\n",
    "    print(f\"âš ï¸  Data directory not found: {data_dir}\")\n",
    "    print(\"Please download the dataset first (see instructions above)\")\n",
    "else:\n",
    "    print(f\"âœ“ Data directory found: {data_dir}\")\n",
    "    print(f\"\\nContents:\")\n",
    "    for item in data_dir.iterdir():\n",
    "        print(f\"  - {item.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore_files",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the structure more deeply\n",
    "def explore_directory_structure(path, max_depth=3, current_depth=0):\n",
    "    \"\"\"Recursively explore directory structure\"\"\"\n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "    \n",
    "    path = Path(path)\n",
    "    indent = \"  \" * current_depth\n",
    "    \n",
    "    for item in sorted(path.iterdir())[:10]:  # Limit to first 10 items\n",
    "        if item.is_dir():\n",
    "            print(f\"{indent}ðŸ“ {item.name}/\")\n",
    "            explore_directory_structure(item, max_depth, current_depth + 1)\n",
    "        else:\n",
    "            print(f\"{indent}ðŸ“„ {item.name}\")\n",
    "\n",
    "if data_dir.exists():\n",
    "    print(\"Directory Structure:\")\n",
    "    explore_directory_structure(data_dir, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_images",
   "metadata": {},
   "source": [
    "## Step 3: Load Images and Labels\n",
    "\n",
    "**Preprocessing Step 2: Loading Images into Numerical Arrays**\n",
    "\n",
    "We need to:\n",
    "1. **Identify all image files** and their corresponding labels (Pokemon types)\n",
    "2. **Load images** using a library like PIL or OpenCV\n",
    "3. **Convert to numpy arrays** for numerical processing\n",
    "4. **Store metadata** (filenames, labels) for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_images_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pokemon_dataset(data_dir, img_size=(120, 120)):\n",
    "    \"\"\"\n",
    "    Load Pokemon images and labels from directory structure.\n",
    "    \n",
    "    Assumes structure:\n",
    "    - data_dir/type_name/pokemon_name.png\n",
    "    OR\n",
    "    - data_dir/images/ with corresponding CSV metadata\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dir : Path or str\n",
    "        Root directory containing Pokemon data\n",
    "    img_size : tuple\n",
    "        Target size for resizing images (height, width)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    images : np.ndarray\n",
    "        Array of images (n_samples, height, width, channels)\n",
    "    labels : np.ndarray\n",
    "        Array of label indices\n",
    "    label_names : list\n",
    "        List of label names (Pokemon types)\n",
    "    metadata : pd.DataFrame\n",
    "        DataFrame with image filenames and labels\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    images = []\n",
    "    labels = []\n",
    "    filenames = []\n",
    "    \n",
    "    # Check for CSV metadata file\n",
    "    csv_files = list(data_dir.glob('*.csv'))\n",
    "    \n",
    "    if csv_files:\n",
    "        # Load from CSV metadata\n",
    "        print(\"Loading dataset using CSV metadata...\")\n",
    "        df = pd.read_csv(csv_files[0])\n",
    "        print(f\"CSV columns: {df.columns.tolist()}\")\n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # You'll need to adjust column names based on actual CSV structure\n",
    "        # Common patterns: 'filename', 'image', 'path' for image column\n",
    "        #                  'type', 'label', 'class' for label column\n",
    "        \n",
    "        return None, None, None, df  # Return df for manual inspection\n",
    "    \n",
    "    else:\n",
    "        # Load from directory structure (assumes folders are class labels)\n",
    "        print(\"Loading dataset from directory structure...\")\n",
    "        \n",
    "        # Get all subdirectories (each is a class/type)\n",
    "        class_dirs = [d for d in data_dir.iterdir() if d.is_dir()]\n",
    "        label_names = sorted([d.name for d in class_dirs])\n",
    "        label_to_idx = {name: idx for idx, name in enumerate(label_names)}\n",
    "        \n",
    "        print(f\"Found {len(label_names)} classes: {label_names}\")\n",
    "        \n",
    "        # Load images from each class\n",
    "        for class_dir in tqdm(class_dirs, desc=\"Loading classes\"):\n",
    "            class_name = class_dir.name\n",
    "            label_idx = label_to_idx[class_name]\n",
    "            \n",
    "            # Get all image files in this class directory\n",
    "            image_files = list(class_dir.glob('*.png')) + \\\n",
    "                         list(class_dir.glob('*.jpg')) + \\\n",
    "                         list(class_dir.glob('*.jpeg'))\n",
    "            \n",
    "            for img_file in image_files:\n",
    "                try:\n",
    "                    # Load image\n",
    "                    img = Image.open(img_file).convert('RGB')\n",
    "                    \n",
    "                    # Resize to target size\n",
    "                    img = img.resize(img_size)\n",
    "                    \n",
    "                    # Convert to numpy array\n",
    "                    img_array = np.array(img)\n",
    "                    \n",
    "                    images.append(img_array)\n",
    "                    labels.append(label_idx)\n",
    "                    filenames.append(img_file.name)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {img_file}: {e}\")\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        images = np.array(images)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        # Create metadata DataFrame\n",
    "        metadata = pd.DataFrame({\n",
    "            'filename': filenames,\n",
    "            'label_idx': labels,\n",
    "            'label_name': [label_names[idx] for idx in labels]\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nâœ“ Loaded {len(images)} images\")\n",
    "        print(f\"  Image shape: {images.shape}\")\n",
    "        print(f\"  Labels shape: {labels.shape}\")\n",
    "        \n",
    "        return images, labels, label_names, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "if data_dir.exists():\n",
    "    images, labels, label_names, metadata = load_pokemon_dataset(data_dir, img_size=(120, 120))\n",
    "    \n",
    "    if images is not None:\n",
    "        print(f\"\\nDataset Summary:\")\n",
    "        print(f\"  Total images: {len(images)}\")\n",
    "        print(f\"  Number of classes: {len(label_names)}\")\n",
    "        print(f\"  Classes: {label_names}\")\n",
    "        print(f\"\\nClass distribution:\")\n",
    "        print(metadata['label_name'].value_counts())\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  Dataset uses CSV metadata. Inspect the DataFrame above and modify the loading function accordingly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize_samples",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Sample Images\n",
    "\n",
    "**Preprocessing Step 3: Data Inspection**\n",
    "\n",
    "Before preprocessing, we should visualize samples to understand:\n",
    "- Image quality and consistency\n",
    "- Color distributions\n",
    "- Potential issues (blank images, corrupted files, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "if images is not None:\n",
    "    # Display random samples from each class\n",
    "    n_classes = len(label_names)\n",
    "    samples_per_class = 3\n",
    "    \n",
    "    fig, axes = plt.subplots(n_classes, samples_per_class, \n",
    "                            figsize=(3 * samples_per_class, 3 * n_classes))\n",
    "    \n",
    "    for class_idx, class_name in enumerate(label_names):\n",
    "        # Get indices of images from this class\n",
    "        class_indices = np.where(labels == class_idx)[0]\n",
    "        \n",
    "        # Sample random images\n",
    "        sample_indices = np.random.choice(class_indices, \n",
    "                                         size=min(samples_per_class, len(class_indices)), \n",
    "                                         replace=False)\n",
    "        \n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            ax = axes[class_idx, i] if n_classes > 1 else axes[i]\n",
    "            ax.imshow(images[idx])\n",
    "            if i == 0:\n",
    "                ax.set_ylabel(class_name, fontsize=12)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Sample Images from Each Class', y=1.001, fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing_normalization",
   "metadata": {},
   "source": [
    "## Step 5: Normalize Pixel Values\n",
    "\n",
    "**Preprocessing Step 4: Numerical Normalization**\n",
    "\n",
    "Raw pixel values range from 0-255 (uint8). For neural networks, we need to normalize these to a standard range.\n",
    "\n",
    "### Why normalize?\n",
    "1. **Faster convergence**: Normalized inputs help gradient descent converge faster\n",
    "2. **Numerical stability**: Prevents overflow/underflow in computations\n",
    "3. **Equal feature importance**: All pixels contribute equally to the loss\n",
    "\n",
    "### Common normalization strategies:\n",
    "1. **Min-Max scaling to [0, 1]**: `X / 255.0`\n",
    "2. **Standardization to [-1, 1]**: `(X / 127.5) - 1`\n",
    "3. **Z-score normalization**: `(X - mean) / std` (per-channel or per-image)\n",
    "\n",
    "We'll use **Min-Max scaling** as it's simple and effective for image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normalize",
   "metadata": {},
   "outputs": [],
   "source": [
    "if images is not None:\n",
    "    # Check current data type and range\n",
    "    print(f\"Before normalization:\")\n",
    "    print(f\"  Data type: {images.dtype}\")\n",
    "    print(f\"  Min value: {images.min()}\")\n",
    "    print(f\"  Max value: {images.max()}\")\n",
    "    print(f\"  Mean value: {images.mean():.2f}\")\n",
    "    \n",
    "    # Normalize to [0, 1] range\n",
    "    X = images.astype('float32') / 255.0\n",
    "    y = labels\n",
    "    \n",
    "    print(f\"\\nAfter normalization:\")\n",
    "    print(f\"  Data type: {X.dtype}\")\n",
    "    print(f\"  Min value: {X.min()}\")\n",
    "    print(f\"  Max value: {X.max()}\")\n",
    "    print(f\"  Mean value: {X.mean():.4f}\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Images normalized to [0, 1] range\")\n",
    "    print(f\"  Shape: {X.shape}\")\n",
    "    print(f\"  This represents {X.shape[0]} images of size {X.shape[1]}x{X.shape[2]} with {X.shape[3]} color channels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_test_split",
   "metadata": {},
   "source": [
    "## Step 6: Train/Test Split\n",
    "\n",
    "**Preprocessing Step 5: Data Splitting**\n",
    "\n",
    "We need to split our data into training and testing sets to evaluate model performance on unseen data.\n",
    "\n",
    "### Important considerations:\n",
    "1. **Stratification**: Ensure each split has proportional representation of each class\n",
    "2. **Random seed**: Set a seed for reproducibility\n",
    "3. **Split ratio**: Typically 80/20 or 70/30 for train/test\n",
    "4. **Validation set**: You may want to create a separate validation set or use cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_test_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if images is not None:\n",
    "    # Split data with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,      # 20% for testing\n",
    "        random_state=42,     # Reproducibility\n",
    "        stratify=y           # Maintain class distribution\n",
    "    )\n",
    "    \n",
    "    print(f\"Data split complete:\")\n",
    "    print(f\"  Training set: {X_train.shape[0]} images ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"  Test set:     {X_test.shape[0]} images ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nClass distribution in training set:\")\n",
    "    train_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "    for idx, count in train_counts.items():\n",
    "        print(f\"  {label_names[idx]}: {count}\")\n",
    "    \n",
    "    print(f\"\\nClass distribution in test set:\")\n",
    "    test_counts = pd.Series(y_test).value_counts().sort_index()\n",
    "    for idx, count in test_counts.items():\n",
    "        print(f\"  {label_names[idx]}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_augmentation",
   "metadata": {},
   "source": [
    "## Step 7: Data Augmentation (Optional)\n",
    "\n",
    "**Preprocessing Step 6: Augmentation for Better Generalization**\n",
    "\n",
    "Data augmentation artificially increases dataset size by creating modified versions of images.\n",
    "\n",
    "### Why augment?\n",
    "1. **Prevents overfitting**: Model sees variations of same image\n",
    "2. **Improves generalization**: Model learns to recognize objects from different perspectives\n",
    "3. **Addresses class imbalance**: Can oversample minority classes\n",
    "\n",
    "### Common augmentation techniques:\n",
    "- **Geometric**: Rotation, flipping, shifting, zooming, shearing\n",
    "- **Color**: Brightness, contrast, saturation adjustments\n",
    "- **Noise**: Gaussian noise, blur\n",
    "\n",
    "For Pokemon images, we might use:\n",
    "- Small rotations (Â±15 degrees)\n",
    "- Horizontal flips\n",
    "- Small shifts and zoom\n",
    "- Brightness variations\n",
    "\n",
    "We'll apply augmentation during training using Keras ImageDataGenerator or custom augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "augmentation_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "if images is not None:\n",
    "    # Define augmentation parameters\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=15,           # Randomly rotate images by Â±15 degrees\n",
    "        width_shift_range=0.1,       # Randomly shift images horizontally by 10%\n",
    "        height_shift_range=0.1,      # Randomly shift images vertically by 10%\n",
    "        horizontal_flip=True,        # Randomly flip images horizontally\n",
    "        zoom_range=0.1,              # Randomly zoom images by Â±10%\n",
    "        fill_mode='nearest'          # Fill empty pixels with nearest value\n",
    "    )\n",
    "    \n",
    "    print(\"Data augmentation configured:\")\n",
    "    print(\"  - Rotation: Â±15 degrees\")\n",
    "    print(\"  - Width/Height shift: Â±10%\")\n",
    "    print(\"  - Horizontal flip: Yes\")\n",
    "    print(\"  - Zoom: Â±10%\")\n",
    "    \n",
    "    # Visualize augmented images\n",
    "    if len(X_train) > 0:\n",
    "        sample_img = X_train[0:1]  # Get first image\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        # Original image\n",
    "        axes[0].imshow(sample_img[0])\n",
    "        axes[0].set_title('Original')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Generate 7 augmented versions\n",
    "        i = 1\n",
    "        for batch in datagen.flow(sample_img, batch_size=1):\n",
    "            axes[i].imshow(batch[0])\n",
    "            axes[i].set_title(f'Augmented {i}')\n",
    "            axes[i].axis('off')\n",
    "            i += 1\n",
    "            if i >= 8:\n",
    "                break\n",
    "        \n",
    "        plt.suptitle('Original vs Augmented Images', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_preprocessed",
   "metadata": {},
   "source": [
    "## Step 8: Save Preprocessed Data\n",
    "\n",
    "**Preprocessing Step 7: Saving for Model Training**\n",
    "\n",
    "We'll save the preprocessed data so we can load it quickly in the training notebook without repeating preprocessing.\n",
    "\n",
    "### Saving strategies:\n",
    "1. **NumPy format (.npz)**: Efficient for numerical arrays\n",
    "2. **HDF5 (.h5)**: Good for large datasets with metadata\n",
    "3. **Pickle (.pkl)**: Can save Python objects but less portable\n",
    "\n",
    "We'll use NumPy's compressed format for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "if images is not None:\n",
    "    # Create output directory\n",
    "    output_dir = Path('../data/pokemon_preprocessed')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save preprocessed data\n",
    "    np.savez_compressed(\n",
    "        output_dir / 'pokemon_data.npz',\n",
    "        X_train=X_train,\n",
    "        X_test=X_test,\n",
    "        y_train=y_train,\n",
    "        y_test=y_test,\n",
    "        label_names=label_names\n",
    "    )\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata.to_csv(output_dir / 'metadata.csv', index=False)\n",
    "    \n",
    "    print(f\"âœ“ Data saved to {output_dir}\")\n",
    "    print(f\"  Files created:\")\n",
    "    print(f\"    - pokemon_data.npz (preprocessed arrays)\")\n",
    "    print(f\"    - metadata.csv (image metadata)\")\n",
    "    \n",
    "    # Show file sizes\n",
    "    npz_size = (output_dir / 'pokemon_data.npz').stat().st_size / (1024**2)\n",
    "    csv_size = (output_dir / 'metadata.csv').stat().st_size / 1024\n",
    "    print(f\"\\n  File sizes:\")\n",
    "    print(f\"    - pokemon_data.npz: {npz_size:.2f} MB\")\n",
    "    print(f\"    - metadata.csv: {csv_size:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_preprocessed",
   "metadata": {},
   "source": [
    "## Step 9: Loading Preprocessed Data (for Training Notebook)\n",
    "\n",
    "This cell shows how to load the preprocessed data in your training notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load preprocessed data\n",
    "def load_preprocessed_data(data_path='../data/pokemon_preprocessed/pokemon_data.npz'):\n",
    "    \"\"\"\n",
    "    Load preprocessed Pokemon dataset.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train, X_test, y_train, y_test, label_names\n",
    "    \"\"\"\n",
    "    data = np.load(data_path)\n",
    "    return (\n",
    "        data['X_train'],\n",
    "        data['X_test'],\n",
    "        data['y_train'],\n",
    "        data['y_test'],\n",
    "        data['label_names']\n",
    "    )\n",
    "\n",
    "# Usage in training notebook:\n",
    "# X_train, X_test, y_train, y_test, label_names = load_preprocessed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary of Preprocessing Steps\n",
    "\n",
    "We completed the following preprocessing steps:\n",
    "\n",
    "1. **Dataset Download**: Downloaded Pokemon images and types from Kaggle\n",
    "2. **Structure Exploration**: Understood the organization of images and labels\n",
    "3. **Image Loading**: Loaded images into numpy arrays with consistent dimensions\n",
    "4. **Data Inspection**: Visualized samples to check quality\n",
    "5. **Normalization**: Converted pixel values from [0, 255] to [0, 1] for neural network training\n",
    "6. **Train/Test Split**: Split data with stratification to maintain class distribution\n",
    "7. **Augmentation Setup**: Configured data augmentation for training (rotation, shifts, flips, zoom)\n",
    "8. **Data Saving**: Saved preprocessed arrays in compressed format for efficient loading\n",
    "\n",
    "### Key Numerical Features:\n",
    "\n",
    "Our \"numerical features\" are the normalized pixel values:\n",
    "- Each image is represented as a 3D array: (height, width, channels)\n",
    "- For RGB images of size 120x120: (120, 120, 3) = 43,200 features per image\n",
    "- Values are normalized to [0, 1] range (float32)\n",
    "- This numerical representation is suitable for CNN input\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In the training notebook, you will:\n",
    "1. Load the preprocessed data\n",
    "2. Build a CNN architecture (following the CIFAR-10 pattern)\n",
    "3. Train the model with augmented data\n",
    "4. Evaluate performance on test set\n",
    "5. Visualize results (confusion matrix, training curves, etc.)\n",
    "\n",
    "### Memory Optimization:\n",
    "\n",
    "If you encounter RAM issues:\n",
    "- **Reduce image size**: Use smaller dimensions (e.g., 64x64 instead of 120x120)\n",
    "- **Use grayscale**: Convert to single channel (reduces features by 3x)\n",
    "- **Batch processing**: Load and process images in smaller batches\n",
    "- **Feature extraction**: Use a pre-trained model to extract features instead of raw pixels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
